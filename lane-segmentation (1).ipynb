{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom glob import glob\nfrom scipy.io import loadmat\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = sorted(glob(\"../input/cityscapes-500x500/cityscapes500x500/cityscapes500x500/leftImg8bit/train/*/*Img8bit.png\"))\ntrain_labels = sorted(glob(\"../input/cityscapes-500x500/cityscapes500x500/cityscapes500x500/gtFine/train/*/*labelIds.png\"))\nval_images = sorted(glob(\"../input/cityscapes-500x500/cityscapes500x500/cityscapes500x500/leftImg8bit/val/*/*Img8bit.png\"))\nval_labels = sorted(glob(\"../input/cityscapes-500x500/cityscapes500x500/cityscapes500x500/gtFine/val/*/*labelIds.png\"))\ntest_images = sorted(glob(\"../input/cityscapes-500x500/cityscapes500x500/cityscapes500x500/leftImg8bit/test/berlin/*\"))\nprint(len(train_images))\nprint(len(val_images))\nprint(len(test_images))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\ndataset_val = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\ndataset_train, dataset_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_png(path, channels=3): \n    # path:  image or label path, if image path, channels=3.  if label path, channels=1\n    # because in this training, we read label is  gtFine_labelIds.png(channels=1) ,  not _gtFine_color.png\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=channels)\n    return img\n\ndef crop_img(img, label):\n    concat_img = tf.concat([img, label], axis=-1)\n    concat_img = tf.image.resize(concat_img, (280, 280), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n    crop_img = tf.image.random_crop(concat_img, [256, 256, 4])\n    # tf.image.random_cropï¼Œ crop a tensor to a given size randomly, here is [256, 256, 4]\n    # images and labels are cropped at the same time to maintain consistency,  so need to concat([img, label]) \n    return crop_img[:, :, 0:3], crop_img[:, :, 3:]\n\ndef normal(img, label):\n    img = tf.cast(img, tf.float32)/127.5 -1\n    label = tf.cast(label, tf.int32)\n    return img, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image_train(img_path, label_path):\n    img = read_png(img_path)\n    label = read_png(label_path, channels=1)\n    if tf.random.uniform(()) > 0.5:\n        img = tf.image.flip_left_right(img)\n        label = tf.image.flip_left_right(label) \n    img = tf.image.resize(img, (256, 256))\n    label = tf.image.resize(label, (256, 256))              \n    img, label = normal(img, label)\n    return img, label\n\ndef load_image_val(img_path, label_path):\n    \n    img = read_png(img_path)\n    label = read_png(label_path, channels=1)\n    \n    img = tf.image.resize(img, (256, 256))\n    label = tf.image.resize(label, (256, 256))\n    \n    img, label = normal(img, label)\n    return img, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = np.random.permutation(len(train_images))\ntrain_images = np.array(train_images)[index]\ntrain_labels = np.array(train_labels)[index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nBUFFER_SIZE = 300\ntrain_count = len(train_images)\nval_count = len(val_images)\ntrain_step_per_epoch = train_count // BATCH_SIZE\nval_step_per_epoch = val_count // BATCH_SIZE\nauto = tf.data.experimental.AUTOTUNE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = dataset_train.map(load_image_train, num_parallel_calls=auto)\ndataset_val =dataset_val.map(load_image_val, num_parallel_calls=auto)\n\ndataset_train = dataset_train.cache().repeat().shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(auto)\ndataset_val = dataset_val.cache().batch(BATCH_SIZE, drop_remainder=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train, dataset_val","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convolution_block(\n    block_input,\n    num_filters=256,\n    kernel_size=3,\n    dilation_rate=1,\n    padding=\"same\",\n    use_bias=False,\n):\n    x = layers.Conv2D(\n        num_filters,\n        kernel_size=kernel_size,\n        dilation_rate=dilation_rate,\n        padding=\"same\",\n        use_bias=use_bias,\n        kernel_initializer=keras.initializers.HeNormal(),\n    )(block_input)\n    x = layers.BatchNormalization()(x)\n    return tf.nn.relu(x)\n\ndef DilatedSpatialPyramidPooling(dspp_input):\n    dims = dspp_input.shape\n    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n    x = convolution_block(x, kernel_size=1, use_bias=True)\n    out_pool = layers.UpSampling2D(\n        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n    )(x)\n\n    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n\n    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n    output = convolution_block(x, kernel_size=1)\n    return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE = 256\nNUM_CLASSES = 34\ndef DeeplabV3Plus(image_size, num_classes):\n    model_input = keras.Input(shape=(image_size, image_size, 3))\n    resnet50 = keras.applications.ResNet50(\n         weights='imagenet', \n         include_top=False, input_tensor=model_input\n    )\n    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n    x = DilatedSpatialPyramidPooling(x)\n\n    input_a = layers.UpSampling2D(\n        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]), \n        interpolation=\"bilinear\",\n    )(x)\n    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n\n    x = layers.Concatenate(axis=-1)([input_a, input_b])\n    x = convolution_block(x)\n    x = convolution_block(x)\n    x = layers.UpSampling2D(\n        size=(image_size // x.shape[1], image_size // x.shape[2]),\n        interpolation=\"bilinear\",\n    )(x)\n    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n    return keras.Model(inputs=model_input, outputs=model_output)\n\n\nmodel = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n# model.summary()\ntf.keras.utils.plot_model(model, to_file=\"my_model.png\", show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MeanIoU(tf.keras.metrics.MeanIoU):\n#     def __call__(self, y_true, y_pred, sample_weight=None):\n#         y_pred = tf.argmax(y_pred, axis=-1)\n#         return super().__call__(y_true, y_pred, sample_weight=sample_weight)\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        y_pred = tf.argmax(y_pred, axis=-1)\n        return super().update_state(y_true, y_pred, sample_weight)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def optimizer_adam_v2():\n    # PiecewiseConstantDecay\n    step = tf.Variable(0, trainable=False)\n    boundaries = [2000, 5000]\n    values = [0.0001, 0.00005, 0.000025]\n\n    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n\n    # Later, whenever we perform an optimization step, we pass in the step.\n    learning_rate_adam = learning_rate_fn(step)\n\n    return keras.optimizers.Adam(learning_rate=learning_rate_adam)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_callbacks = [\n    # tf.keras.callbacks.EarlyStopping(\n        # monitor= 'val_mean_io_u', \n        # min_delta=0, \n        # patience=10, \n        # verbose= 1, \n        # mode='auto', \n        # restore_best_weights=True),\n    keras.callbacks.ModelCheckpoint(\n        filepath='./{epoch:02d}-{val_loss:.2f}.h5',\n        monitor='val_loss',\n        save_best_only=False,\n        save_weights_only=False)\n    # keras.callbacks.LearningRateScheduler(scheduler),\n]\n\nloss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(\n    optimizer=optimizer_adam_v2(),\n    loss=loss,\n    metrics=['acc', MeanIoU(num_classes=34)]\n    # metrics=['accuracy']\n)\n\nstart = time.time()\n# model.load_weights('../input/best-weight/43-0.97.h5')\nhistory = model.fit(dataset_train, \n                   epochs= 20,\n                   validation_data=dataset_val,\n                   steps_per_epoch=train_step_per_epoch,\n                   validation_steps=val_step_per_epoch,\n#                    callbacks=[my_callbacks],\n                   verbose= 1)\nend =time.time()\nprint(str(int(end - start)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the Colormap\ncolormap = loadmat(\n    \"../input/colormap/CS_colormap.mat\"\n)[\"colormap\"]\ncolormap = colormap * 100\ncolormap = colormap.astype(np.uint8)\n\n\ndef infer(model, image_tensor):\n    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n    predictions = np.squeeze(predictions)\n    predictions = np.argmax(predictions, axis=2)\n    return predictions\n\n\ndef decode_segmentation_masks(mask, colormap, n_classes):\n    r = np.zeros_like(mask).astype(np.uint8)\n    g = np.zeros_like(mask).astype(np.uint8)\n    b = np.zeros_like(mask).astype(np.uint8)\n    for l in range(n_classes):\n        idx = mask == l\n        r[idx] = colormap[l, 0]\n        g[idx] = colormap[l, 1]\n        b[idx] = colormap[l, 2]\n    rgb = np.stack([r, g, b], axis=2)\n    return rgb\n\n\ndef get_overlay(image, colored_mask):\n    image = tf.keras.preprocessing.image.array_to_img(image)\n    image = np.array(image).astype(np.uint8)\n    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n    return overlay\n\n\ndef plot_samples_matplotlib(display_list, figsize=(5, 3)):\n    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n    for i in range(len(display_list)):\n        if display_list[i].shape[-1] == 3:\n            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        else:\n            axes[i].imshow(display_list[i])\n    plt.show()\n\n\ndef plot_predictions(images_list, colormap, model):\n    for image_file in images_list:\n        image_tensor = tf.cast(read_png(image_file), tf.float32)/127.5 -1\n        image_tensor = tf.image.resize(images=image_tensor, size=[128, 128])\n        prediction_mask = infer(image_tensor=image_tensor, model=model)\n        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 34)\n        overlay = get_overlay(image_tensor, prediction_colormap)\n        plot_samples_matplotlib(\n            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n        )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_model = tf.keras.models.load_model('my_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = sorted(glob(\"../input/cityscapes-500x500/cityscapes500x500/cityscapes500x500/leftImg8bit/test/berlin/*\"))\nplot_predictions(test_images[10:15], colormap, model=model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the Colormap\ncolormap = loadmat(\n    \"../input/colormap/CS_colormap.mat\"\n)[\"colormap\"]\ncolormap = colormap * 100\ncolormap = colormap.astype(np.uint8)\n\n\ndef infer(model, image_tensor):\n    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n    predictions = np.squeeze(predictions)\n    predictions = np.argmax(predictions, axis=2)\n    return predictions\n\n\ndef decode_segmentation_masks(mask, colormap, n_classes):\n    r = np.zeros_like(mask).astype(np.uint8)\n    g = np.zeros_like(mask).astype(np.uint8)\n    b = np.zeros_like(mask).astype(np.uint8)\n    for l in range(n_classes):\n        idx = mask == l\n        r[idx] = colormap[l, 0]\n        g[idx] = colormap[l, 1]\n        b[idx] = colormap[l, 2]\n    rgb = np.stack([r, g, b], axis=2)\n    return rgb\n\n\ndef get_overlay2(image, colored_mask):\n    image = tf.keras.preprocessing.image.array_to_img(image)\n    image = np.array(image).astype(np.uint8)\n    overlay = cv2.addWeighted(image, 0.1, colored_mask, 0.7, 0)\n    return overlay\n\n\ndef plot_samples_matplotlib(display_list, figsize=(5, 3)):\n    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n    for i in range(len(display_list)):\n        if display_list[i].shape[-1] == 3:\n            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        else:\n            axes[i].imshow(display_list[i])\n    plt.show()\n\n\ndef plot_predictions(images_list, colormap, model):\n    for image_file in images_list:\n        image_tensor = read_image(image_file)\n        image_tensor = tf.image.resize(images=image_tensor, size=[128, 128])\n        prediction_mask = infer(image_tensor=image_tensor, model=model)\n        prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 34)\n        overlay = get_overlay2(image_tensor, prediction_colormap)\n        plot_samples_matplotlib(\n            [image_tensor, overlay, prediction_colormap], figsize=(18, 14)\n        )\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport cv2 as cv\n\ndef rescale_frame(frame_input):\n    width = 640\n    height = 360\n    dim = (width, height)\n    return cv2.resize(frame_input, dim, interpolation=cv2.INTER_AREA)\n\ndef overlayframe(frame, model, colormap):\n    frame = tf.image.resize(images=frame, size=[256, 256])\n    frame = tf.cast(frame, tf.float32)/127.5 -1\n    prediction_mask = infer(image_tensor=frame, model=model)\n    prediction_colormap = decode_segmentation_masks(prediction_mask, colormap, 34)\n    overlay = get_overlay2(frame, prediction_colormap)\n    overlay = rescale_frame(overlay)\n#     overlay = tf.cast(overlay, tf.float32)\n    return overlay\n\ncap = cv.VideoCapture('../input/inputavi/input.avi')\n\nfourcc = cv.VideoWriter_fourcc(*'XVID')\nout = cv.VideoWriter('output.avi',fourcc, 10.0, (640,360))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        print(\"Can't receive frame (stream end?). Exiting ...\")\n        break\n    frame = overlayframe(frame, model, colormap)\n    out.write(frame)\n    # frame = cv.flip(frame, 0)\n    # out.write(frame)\n\ncap.release()\nout.release()\n# cv.destroyAllWindows()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}